diff --git a/caiman/components_evaluation.py b/caiman/components_evaluation.py
index 5bf9a47c..53315d02 100644
--- a/caiman/components_evaluation.py
+++ b/caiman/components_evaluation.py
@@ -6,7 +6,6 @@ import logging
 import numpy as np
 import os
 import peakutils
-import tensorflow as tf
 import scipy
 from scipy.sparse import csc_matrix
 from scipy.stats import norm
@@ -274,6 +273,7 @@ def evaluate_components_CNN(A,
         print("GPU run not requested, disabling use of GPUs")
         os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
     try:
+        import tensorflow as tf
         os.environ["KERAS_BACKEND"] = "tensorflow"
         from tensorflow.keras.models import model_from_json
         use_keras = True
@@ -323,6 +323,7 @@ def evaluate_components_CNN(A,
     if use_keras:
         predictions = loaded_model.predict(final_crops[:, :, :, np.newaxis], batch_size=32, verbose=1)
     else:
+        import tensorflow as tf
         tf_in = loaded_model.get_tensor_by_name('prefix/conv2d_20_input:0')
         tf_out = loaded_model.get_tensor_by_name('prefix/output_node0:0')
         with tf.Session(graph=loaded_model) as sess:
diff --git a/caiman/source_extraction/cnmf/online_cnmf.py b/caiman/source_extraction/cnmf/online_cnmf.py
index c84ec3f5..b4245e02 100644
--- a/caiman/source_extraction/cnmf/online_cnmf.py
+++ b/caiman/source_extraction/cnmf/online_cnmf.py
@@ -8,9 +8,9 @@ is storead in an Estimates class
 
 More info:
 ------------
-Giovannucci, A., Friedrich, J., Kaufman, M., Churchland, A., Chklovskii, D., 
+Giovannucci, A., Friedrich, J., Kaufman, M., Churchland, A., Chklovskii, D.,
 Paninski, L., & Pnevmatikakis, E.A. (2017). OnACID: Online analysis of calcium
-imaging data in real time. In Advances in Neural Information Processing Systems 
+imaging data in real time. In Advances in Neural Information Processing Systems
 (pp. 2381-2391).
 @url http://papers.nips.cc/paper/6832-onacid-online-analysis-of-calcium-imaging-data-in-real-time
 """
@@ -26,7 +26,6 @@ from scipy.sparse import coo_matrix, csc_matrix, spdiags, hstack
 from scipy.stats import norm
 from sklearn.decomposition import NMF
 from sklearn.preprocessing import normalize
-import tensorflow as tf
 from time import time
 
 import caiman
@@ -49,7 +48,6 @@ from caiman.source_extraction.cnmf.utilities import (update_order, peak_local_ma
 import caiman.summary_images
 from caiman.utils.utils import save_dict_to_hdf5, load_dict_from_hdf5, parmap, load_graph
 from caiman.utils.stats import pd_solve
-from caiman.utils.nn_models import (fit_NL_model, create_LN_model, quantile_loss, rate_scheduler)
 
 try:
     cv2.setNumThreads(0)
@@ -70,11 +68,11 @@ class OnACID(object):
     state of the algorithm (optional)
 
     Methods:
-        initialize_online: 
+        initialize_online:
             Initialize the online algorithm using a provided method, and prepare
             the online object
 
-        _prepare_object: 
+        _prepare_object:
             Prepare the online object given a set of estimates
 
         fit_next:
@@ -89,7 +87,7 @@ class OnACID(object):
 
         Args:
             params: CNMFParams
-                CNMFParams object with parameters that are used to perform online motion correction, followed by online CNMF 
+                CNMFParams object with parameters that are used to perform online motion correction, followed by online CNMF
 
             estimates: Estimates, optional
                 Estimates object to load an existing model
@@ -122,7 +120,7 @@ class OnACID(object):
         logger = logging.getLogger("caiman")
         init_batch = self.params.get('online', 'init_batch')
         old_dims = self.params.get('data', 'dims')
-        self.is1p = (self.params.get('init', 'method_init') == 'corr_pnr' and 
+        self.is1p = (self.params.get('init', 'method_init') == 'corr_pnr' and
                     self.params.get('init', 'ring_size_factor') is not None)
 
         if idx_components is None:
@@ -141,9 +139,9 @@ class OnACID(object):
 
         if not self.params.get('online', 'update_num_comps'):
             self.params.set('online', {'expected_comps': self.N})
-        elif (self.params.get('online', 'expected_comps') <= 
+        elif (self.params.get('online', 'expected_comps') <=
             self.N + self.params.get('online', 'max_num_added')):
-            self.params.set('online', {'expected_comps': self.N + 
+            self.params.set('online', {'expected_comps': self.N +
                 self.params.get('online', 'max_num_added') + 200})
         expected_comps = self.params.get('online', 'expected_comps')
 
@@ -212,7 +210,7 @@ class OnACID(object):
                 self.estimates.C_on[i, :init_batch] = o.c
         else:
             self.estimates.C_on[:self.N, :init_batch] = self.estimates.C
-        
+
         if self.is1p:
             ssub_B = self.params.get('init', 'ssub_B') * self.params.get('init', 'ssub')
             X = Yr[:, :init_batch] - np.asarray(self.estimates.A.dot(self.estimates.C))
@@ -270,7 +268,7 @@ class OnACID(object):
 
         if self.is1p:
             estim = self.estimates
-            d1, d2 = estim.dims    
+            d1, d2 = estim.dims
             estim.Yres_buf -= estim.b0
             if ssub_B == 1:
                 estim.Atb = estim.Ab.T.dot(estim.W.dot(estim.b0) - estim.b0)
@@ -343,6 +341,8 @@ class OnACID(object):
                 self.tf_in = None
                 self.tf_out = None
             else:
+                # Lazyload tensorflow here
+                import tensorflow as tf
                 path = self.params.get('online', 'path_to_model').split(".")[:-1]
                 model_path = '.'.join(path + ['h5', 'pb'])
                 loaded_model = load_graph(model_path)
@@ -442,7 +442,7 @@ class OnACID(object):
                 self.estimates.C_on[:self.M, t], self.estimates.noisyC[:self.M, t] = demix1p(
                     frame, self.estimates.Ab, C_in, self.estimates.AtA, Atb=self.estimates.Atb,
                     AtW=self.estimates.AtW, AtWA=self.estimates.AtWA, iters=num_iters_hals,
-                    groups=self.estimates.groups, ssub_B=ssub_B, 
+                    groups=self.estimates.groups, ssub_B=ssub_B,
                     downscale_matrix=self.estimates.downscale_matrix if ssub_B > 1 else None)
             else:
                 self.estimates.C_on[:self.M, t], self.estimates.noisyC[:self.M, t] = HALS4activity(
@@ -486,7 +486,7 @@ class OnACID(object):
         self.estimates.mn = (t-1)/t*self.estimates.mn + res_frame/t
         self.estimates.vr = (t-1)/t*self.estimates.vr + (res_frame - mn_)*(res_frame - self.estimates.mn)/t
         self.estimates.sn = np.sqrt(self.estimates.vr)
-        
+
         t_new = time()
         num_added = 0
         if self.params.get('online', 'update_num_comps'):
@@ -494,7 +494,7 @@ class OnACID(object):
             if self.params.get('online', 'use_corr_img'):
                 corr_img_mode = 'simple'  #'exponential'  # 'cumulative'
                 self.estimates.corr_img = caiman.summary_images.update_local_correlations(
-                    t + 1 if corr_img_mode == 'cumulative' else mbs, 
+                    t + 1 if corr_img_mode == 'cumulative' else mbs,
                     res_frame.reshape((1,) + self.estimates.dims, order='F'),
                     self.estimates.first_moment, self.estimates.second_moment,
                     self.estimates.crosscorr, self.estimates.col_ind, self.estimates.row_ind,
@@ -536,7 +536,7 @@ class OnACID(object):
                 thresh_fitness_raw=self.params.get('online', 'thresh_fitness_raw'),
                 thresh_overlap=self.params.get('online', 'thresh_overlap'), groups=self.estimates.groups,
                 batch_update_suff_stat=self.params.get('online', 'batch_update_suff_stat'),
-                gnb=self.params.get('init', 'nb'), sn=self.estimates.sn, 
+                gnb=self.params.get('init', 'nb'), sn=self.estimates.sn,
                 g=g_est, s_min=self.params.get('temporal', 's_min'),
                 Ab_dense=self.estimates.Ab_dense if self.params.get('online', 'use_dense') else None,
                 oases=self.estimates.OASISinstances if self.params.get('preprocess', 'p') else None,
@@ -613,7 +613,7 @@ class OnACID(object):
                     self.estimates.AtA[:, -num_added:] = self.estimates.Ab.T.dot(
                         self.estimates.Ab[:, -num_added:]).toarray()
                 self.estimates.AtA[-num_added:] = self.estimates.AtA[:, -num_added:].T
-                
+
                 if self.is1p:
                     # # update XXt and W: TODO only update necessary pixels not all!
 
@@ -780,7 +780,7 @@ class OnACID(object):
                     else:
                         XXt_mats = self.XXt_mats
                         XXt_vecs = self.XXt_vecs
-                        if self.dview is None: 
+                        if self.dview is None:
                             W.data = np.concatenate(list(map(inv_mat_vec, zip(XXt_mats, XXt_vecs))))
                         elif 'multiprocessing' in str(type(self.dview)):
                             W.data = np.concatenate(list(self.dview.imap(inv_mat_vec, zip(XXt_mats, XXt_vecs), chunksize=256)))
@@ -1031,7 +1031,7 @@ class OnACID(object):
         frame_ = frame.flatten(order='F')
         if self.is1p and self.estimates.W is not None:
             templ = self.estimates.Ab.dot(
-                np.median(self.estimates.C_on[:self.M, t-50:t], 1))            
+                np.median(self.estimates.C_on[:self.M, t-50:t], 1))
             if self.params.get('init','ssub_B') == 1:
                 B = self.estimates.W.dot(frame_ - templ - self.estimates.b0) + self.estimates.b0
             else:
@@ -1109,6 +1109,8 @@ class OnACID(object):
         Returns:
             self (results of caiman online)
         """
+        # LazyLoad here to avoid hard dependency on tensorflow
+        from caiman.utils.nn_models import (fit_NL_model, create_LN_model, quantile_loss, rate_scheduler)
         logger = logging.getLogger("caiman")
         self.t_init = -time()
         fls = self.params.get('data', 'fnames')
@@ -1230,7 +1232,7 @@ class OnACID(object):
                             templ = None
                             frame_cor = frame_
                         self.t_motion.append(time() - t_mot)
-                        
+
                         if self.params.get('online', 'normalize'):
                             frame_cor = frame_cor/self.img_norm
                         # Fit next frame
@@ -1253,7 +1255,7 @@ class OnACID(object):
                         t_online.append(time() - t_frame_start)
                     except  (StopIteration, RuntimeError):
                         break
-        
+
             self.Ab_epoch.append(self.estimates.Ab.copy())
 
         if self.params.get('online', 'normalize'):
@@ -1345,7 +1347,7 @@ class OnACID(object):
                                                   # spatial shapes
         frame_comp_1 = cv2.resize(np.concatenate([frame_plot, all_comps * 1.], axis=-1),
                                   (2 * int(self.dims[1] * resize_fact), int(self.dims[0] * resize_fact)))
-        frame_comp_2 = cv2.resize(np.concatenate([comps_frame, denoised_frame], axis=-1), 
+        frame_comp_2 = cv2.resize(np.concatenate([comps_frame, denoised_frame], axis=-1),
                                   (2 * int(self.dims[1] * resize_fact), int(self.dims[0] * resize_fact)))
         frame_pn = np.concatenate([frame_comp_1, frame_comp_2], axis=0).T
         if transpose:
@@ -1838,7 +1840,7 @@ def update_shapes(CY, CC, Ab, ind_A, sn=None, q=0.5, indicator_components=None,
             else:
                 for m in idx_comp:  # neurons
                     ind_pixels = ind_A[m - nb]
-                    tmp = np.maximum(Ab_dense[ind_pixels, m] + 
+                    tmp = np.maximum(Ab_dense[ind_pixels, m] +
                         ((CY[m, ind_pixels] - Ab_dense[ind_pixels].dot(CC[m])) / (CC[m, m] + np.finfo(CC.dtype).eps)), 0)
                     # normalize
                     if tmp.dot(tmp) > 0:
@@ -2321,10 +2323,10 @@ def update_num_components(t, sv, Ab, Cf, Yres_buf, Y_buf, rho_buf,
                 # # max_img[[slice(*i) for i in ijSig]] = first_moment[indices].reshape(
                 # #     np.diff(ijSig).ravel(), order='F')
 
-                
+
             Yres_buf[:, indices] -= np.outer(cin, ain)
 
-            
+
             if corr_img is None:
                 # restrict blurring to region where component is located
                 # update bigger region than neural patch to avoid boundary effects
diff --git a/caiman/source_extraction/volpy/mrcnn/model.py b/caiman/source_extraction/volpy/mrcnn/model.py
index 2dee5f23..c3f95ed7 100644
--- a/caiman/source_extraction/volpy/mrcnn/model.py
+++ b/caiman/source_extraction/volpy/mrcnn/model.py
@@ -1110,7 +1110,7 @@ def rpn_bbox_loss_graph(config, target_bbox, rpn_match, rpn_bbox):
                                    config.IMAGES_PER_GPU)
 
     loss = smooth_l1_loss(target_bbox, rpn_bbox)
-    
+
     loss = K.switch(tf.size(input=loss) > 0, K.mean(loss), tf.constant(0.0))
     return loss
 
diff --git a/caiman/source_extraction/volpy/mrcnn/utils.py b/caiman/source_extraction/volpy/mrcnn/utils.py
index 716ef347..506b52af 100644
--- a/caiman/source_extraction/volpy/mrcnn/utils.py
+++ b/caiman/source_extraction/volpy/mrcnn/utils.py
@@ -13,7 +13,6 @@ import logging
 import math
 import random
 import numpy as np
-import tensorflow as tf
 import scipy
 import skimage.color
 import skimage.io
@@ -101,7 +100,7 @@ def compute_overlaps_masks(masks1, masks2):
     """Computes IoU overlaps between two sets of masks.
     masks1, masks2: [Height, Width, instances]
     """
-    
+
     # If either set of masks is empty return empty result
     if masks1.shape[-1] == 0 or masks2.shape[-1] == 0:
         return np.zeros((masks1.shape[-1], masks2.shape[-1]))
@@ -184,6 +183,8 @@ def box_refinement_graph(box, gt_box):
     """Compute refinement needed to transform box to gt_box.
     box and gt_box are [N, (y1, x1, y2, x2)]
     """
+    # Lazyload tensorflow to avoid a hard dependency
+    import tensorflow as tf
     box = tf.cast(box, tf.float32)
     gt_box = tf.cast(gt_box, tf.float32)
 
@@ -757,7 +758,7 @@ def compute_ap_range(gt_box, gt_class_id, gt_mask,
     """Compute AP over a range or IoU thresholds. Default range is 0.5-0.95."""
     # Default is 0.5 to 0.95 with increments of 0.05
     iou_thresholds = iou_thresholds or np.arange(0.5, 1.0, 0.05)
-    
+
     # Compute AP over range of IoU thresholds
     AP = []
     for iou_threshold in iou_thresholds:
@@ -811,6 +812,8 @@ def batch_slice(inputs, graph_fn, batch_size, names=None):
     batch_size: number of slices to divide the data into.
     names: If provided, assigns names to the resulting tensors.
     """
+    # Lazyload tensorflow to avoid a hard dependency
+    import tensorflow as tf
     if not isinstance(inputs, list):
         inputs = [inputs]
 
diff --git a/caiman/source_extraction/volpy/utils.py b/caiman/source_extraction/volpy/utils.py
index 9501fb1c..2d02ff05 100644
--- a/caiman/source_extraction/volpy/utils.py
+++ b/caiman/source_extraction/volpy/utils.py
@@ -4,13 +4,12 @@ Created on Mon Mar 23 16:45:00 2020
 This file create functions used for demo_pipeline_voltage_imaging.py
 @author: caichangjia
 """
-#%% 
+#%%
 from IPython import get_ipython
 import matplotlib.pyplot as plt
 from matplotlib.widgets import Slider
 import numpy as np
 import os
-import tensorflow as tf
 import caiman as cm
 from caiman.external.cell_magic_wand import cell_magic_wand_single_point
 from caiman.paths import caiman_datadir
@@ -20,19 +19,19 @@ def quick_annotation(img, min_radius, max_radius, roughness=2):
     Args:
         img: 2-D array
             img as the background for selection
-            
+
         min_radius: float
             minimum radius of the selection
-            
+
         max_radius: float
             maximum raidus of the selection
-            
+
         roughness: int
             roughness of the selection surface
-            
+
     Return:
         ROIs: 3-D array
-            region of interests 
+            region of interests
             (# of components * # of pixels in x dim * # of pixels in y dim)
     """
     try:
@@ -45,29 +44,29 @@ def quick_annotation(img, min_radius, max_radius, roughness=2):
         print(s)
         plt.title(s, fontsize=16)
         plt.draw()
-        
+
     keep_select=True
     ROIs = []
     while keep_select:
         # Plot img
         plt.clf()
-        plt.imshow(img, cmap='gray', vmax=np.percentile(img, 99))            
+        plt.imshow(img, cmap='gray', vmax=np.percentile(img, 99))
         if len(ROIs) == 0:
             pass
         elif len(ROIs) == 1:
             plt.imshow(ROIs[0], alpha=0.3, cmap='Oranges')
         else:
             plt.imshow(np.array(ROIs).sum(axis=0), alpha=0.3, cmap='Oranges')
-        
+
         # Plot point and ROI
         tellme('Click center of neuron')
         center = plt.ginput(1)[0]
         plt.plot(center[0], center[1], 'r+')
-        ROI = cell_magic_wand_single_point(img, (center[1], center[0]), 
-                                           min_radius=min_radius, max_radius=max_radius, 
+        ROI = cell_magic_wand_single_point(img, (center[1], center[0]),
+                                           min_radius=min_radius, max_radius=max_radius,
                                            roughness=roughness, zoom_factor=1)[0]
         plt.imshow(ROI, alpha=0.3, cmap='Reds')
-    
+
         # Select or not
         tellme('Select? Key click for yes, mouse click for no')
         select = plt.waitforbuttonpress()
@@ -77,10 +76,10 @@ def quick_annotation(img, min_radius, max_radius, roughness=2):
         else:
             tellme('You did not select a neuron \n Keep selecting? Key click for yes, mouse click for no')
         keep_select = plt.waitforbuttonpress()
-        
-    plt.close()        
-    ROIs = np.array(ROIs)   
-    
+
+    plt.close()
+    ROIs = np.array(ROIs)
+
     try:
         if __IPYTHON__:
             get_ipython().run_line_magic('matplotlib', 'inline')
@@ -91,26 +90,28 @@ def quick_annotation(img, min_radius, max_radius, roughness=2):
 
 def mrcnn_inference(img, size_range, weights_path, display_result=True):
     """ Mask R-CNN inference in VolPy
-    Args: 
+    Args:
         img: 2-D array
             summary images for detection
-            
+
         size_range: list
             range of neuron size for selection
-            
+
         weights_path: str
             path for Mask R-CNN weight
-            
+
         display_result: boolean
             if True, the function will plot the result of inference
-        
+
     Return:
         ROIs: 3-D array
-            region of interests 
+            region of interests
             (# of components * # of pixels in x dim * # of pixels in y dim)
     """
     from caiman.source_extraction.volpy.mrcnn import visualize, neurons
     import caiman.source_extraction.volpy.mrcnn.model as modellib
+    # Lazyload tensorflow to avoid a hard dependency on it
+    import tensorflow as tf
     config = neurons.NeuronsConfig()
     class InferenceConfig(config.__class__):
         # Run detection on one img at a time
@@ -131,7 +132,7 @@ def mrcnn_inference(img, size_range, weights_path, display_result=True):
     tf.keras.Model.load_weights(model.keras_model, weights_path, by_name=True)
     results = model.detect([img], verbose=1)
     r = results[0]
-    selection = np.logical_and(r['masks'].sum(axis=(0,1)) > size_range[0] ** 2, 
+    selection = np.logical_and(r['masks'].sum(axis=(0,1)) > size_range[0] ** 2,
                                r['masks'].sum(axis=(0,1)) < size_range[1] ** 2)
     r['rois'] = r['rois'][selection]
     r['masks'] = r['masks'][:, :, selection]
@@ -141,31 +142,31 @@ def mrcnn_inference(img, size_range, weights_path, display_result=True):
 
     if display_result:
         _, ax = plt.subplots(1,1, figsize=(16,16))
-        visualize.display_instances(img, r['rois'], r['masks'], r['class_ids'], 
+        visualize.display_instances(img, r['rois'], r['masks'], r['class_ids'],
                                 ['BG', 'neurons'], r['scores'], ax=ax,
-                                title="Predictions")        
+                                title="Predictions")
     return ROIs
 
 def reconstructed_movie(estimates, fnames, idx, scope, flip_signal):
-    """ Create reconstructed movie in VolPy. The movie has three panels: 
+    """ Create reconstructed movie in VolPy. The movie has three panels:
     motion corrected movie on the left panel, movie removed from the baseline
     on the mid panel and reconstructed movie on the right panel.
-    Args: 
+    Args:
         estimates: dict
             estimates dictionary contain results of VolPy
-            
+
         fnames: list
             motion corrected movie in F-order memory mapping format
-            
+
         idx: list
             index of selected neurons
-            
+
         scope: list
             scope of number of frames in reconstructed movie
-            
+
         flip_signal: boolean
-            if True the signal will be flipped (for voltron) 
-    
+            if True the signal will be flipped (for voltron)
+
     Return:
         mv_all: 3-D array
             motion corrected movie, movie removed from baseline, reconstructed movie
@@ -183,13 +184,13 @@ def reconstructed_movie(estimates, fnames, idx, scope, flip_signal):
     mv_bl = (mv_bl - mv_bl.min())/(mv_bl.max()-mv_bl.min())
 
     # reconstructed movie
-    estimates['weights'][estimates['weights']<0] = 0    
+    estimates['weights'][estimates['weights']<0] = 0
     A = estimates['weights'][idx].transpose([1,2,0]).reshape((-1,len(idx)))
     C = estimates['t_rec'][idx,scope[0]:scope[1]]
-    mv_rec = np.dot(A, C).reshape((dims[0],dims[1],scope[1]-scope[0])).transpose((2,0,1))    
+    mv_rec = np.dot(A, C).reshape((dims[0],dims[1],scope[1]-scope[0])).transpose((2,0,1))
     mv_rec = cm.movie(mv_rec,fr=400)
     mv_rec = (mv_rec - mv_rec.min())/(mv_rec.max()-mv_rec.min())
-    mv_all = cm.concatenate((mv,mv_bl,mv_rec),axis=2)    
+    mv_all = cm.concatenate((mv,mv_bl,mv_rec),axis=2)
     return mv_all
 
 def view_components(estimates, img, idx, frame_times=None, gt_times=None):
@@ -197,27 +198,27 @@ def view_components(estimates, img, idx, frame_times=None, gt_times=None):
     Args:
         estimates: dict
             estimates dictionary contain results of VolPy
-            
+
         img: 2-D array
             summary images for detection
-            
+
         idx: list
             index of selected neurons
     """
-    n = len(idx) 
+    n = len(idx)
     fig = plt.figure(figsize=(10, 10))
 
     axcomp = plt.axes([0.05, 0.05, 0.9, 0.03])
     ax1 = plt.axes([0.05, 0.55, 0.4, 0.4])
     ax3 = plt.axes([0.55, 0.55, 0.4, 0.4])
-    ax2 = plt.axes([0.05, 0.1, 0.9, 0.4])    
+    ax2 = plt.axes([0.05, 0.1, 0.9, 0.4])
     s_comp = Slider(axcomp, 'Component', 0, n, valinit=0)
     vmax = np.percentile(img, 98)
     if frame_times is not None:
         pass
     else:
         frame_times = np.array(range(len(estimates['t'][0])))
-    
+
     def arrow_key_image_control(event):
 
         if event.key == 'left':
@@ -229,24 +230,24 @@ def view_components(estimates, img, idx, frame_times=None, gt_times=None):
         elif event.key == 'right':
             new_val = np.round(s_comp.val + 1)
             if new_val > n :
-                new_val = n  
+                new_val = n
             s_comp.set_val(new_val)
-        
+
     def update(val):
         i = int(np.round(s_comp.val))
         print(f'Component:{i}')
 
         if i < n:
-            
+
             ax1.cla()
             imgtmp = estimates['weights'][idx][i]
             ax1.imshow(imgtmp, interpolation='None', cmap=plt.cm.gray, vmax=np.max(imgtmp)*0.5, vmin=0)
             ax1.set_title(f'Spatial component {i+1}')
             ax1.axis('off')
-            
+
             ax2.cla()
             ax2.plot(frame_times, estimates['t'][idx][i], alpha=0.8)
-            ax2.plot(frame_times, estimates['t_sub'][idx][i])            
+            ax2.plot(frame_times, estimates['t_sub'][idx][i])
             ax2.plot(frame_times, estimates['t_rec'][idx][i], alpha = 0.4, color='red')
             ax2.plot(frame_times[estimates['spikes'][idx[i]]],
                      1.05 * np.max(estimates['t'][idx][i]) * np.ones(estimates['spikes'][idx[i]].shape),
@@ -260,9 +261,9 @@ def view_components(estimates, img, idx, frame_times=None, gt_times=None):
                 ax2.legend(labels=['t', 't_sub', 't_rec', 'spikes'])
             ax2.set_title(f'Signal and spike times {i+1}')
             ax2.text(0.1, 0.1, f'snr:{round(estimates["snr"][idx][i],2)}', horizontalalignment='center', verticalalignment='center', transform = ax2.transAxes)
-            ax2.text(0.1, 0.07, f'num_spikes: {len(estimates["spikes"][idx[i]])}', horizontalalignment='center', verticalalignment='center', transform = ax2.transAxes)            
-            ax2.text(0.1, 0.04, f'locality_test: {estimates["locality"][idx][i]}', horizontalalignment='center', verticalalignment='center', transform = ax2.transAxes)            
-            
+            ax2.text(0.1, 0.07, f'num_spikes: {len(estimates["spikes"][idx[i]])}', horizontalalignment='center', verticalalignment='center', transform = ax2.transAxes)
+            ax2.text(0.1, 0.04, f'locality_test: {estimates["locality"][idx][i]}', horizontalalignment='center', verticalalignment='center', transform = ax2.transAxes)
+
             ax3.cla()
             ax3.imshow(img, interpolation='None', cmap=plt.cm.gray, vmax=vmax)
             imgtmp2 = imgtmp.copy()
@@ -270,9 +271,9 @@ def view_components(estimates, img, idx, frame_times=None, gt_times=None):
             ax3.imshow(imgtmp2, interpolation='None',
                        alpha=0.5, cmap=plt.cm.hot)
             ax3.axis('off')
-            
+
     s_comp.on_changed(update)
     s_comp.set_val(0)
     fig.canvas.mpl_connect('key_release_event', arrow_key_image_control)
     plt.show()
-    
\ No newline at end of file
+
diff --git a/caiman/tests/test_mrcnn.py b/caiman/tests/test_mrcnn.py
index a39576f2..80346ed9 100644
--- a/caiman/tests/test_mrcnn.py
+++ b/caiman/tests/test_mrcnn.py
@@ -2,7 +2,6 @@
 
 import numpy as np
 import os
-import tensorflow as tf
 
 import caiman as cm
 from caiman.paths import caiman_datadir
@@ -10,13 +9,15 @@ from caiman.utils.utils import download_model, download_demo
 from caiman.source_extraction.volpy.mrcnn import neurons
 import caiman.source_extraction.volpy.mrcnn.model as modellib
 
-# mrcnn disables eager execution during its import, making a lot of later tests unhappy because
-# under nose, all tests run under the same process. Non-eager execution sends modern tensorflow down
-# some rare code paths.
-# It apparently doesn't even need to do this. So let's just undo it immediately
-tf.compat.v1.enable_eager_execution()
 
 def mrcnn(img, size_range, weights_path):
+    import tensorflow as tf
+    # mrcnn disables eager execution during its import, making a lot of later tests unhappy because
+    # under nose, all tests run under the same process. Non-eager execution sends modern tensorflow down
+    # some rare code paths.
+    # It apparently doesn't even need to do this. So let's just undo it immediately
+    tf.compat.v1.enable_eager_execution()
+
     config = neurons.NeuronsConfig()
     class InferenceConfig(config.__class__):
         # Run detection on one img at a time
@@ -37,17 +38,15 @@ def mrcnn(img, size_range, weights_path):
         tf.keras.Model.load_weights(model.keras_model, weights_path, by_name=True)
         results = model.detect([img], verbose=1)
         r = results[0]
-        selection = np.logical_and(r['masks'].sum(axis=(0,1)) > size_range[0] ** 2, 
+        selection = np.logical_and(r['masks'].sum(axis=(0,1)) > size_range[0] ** 2,
                                    r['masks'].sum(axis=(0,1)) < size_range[1] ** 2)
         r['masks'] = r['masks'][:, :, selection]
-        ROIs = r['masks'].transpose([2, 0, 1])   
+        ROIs = r['masks'].transpose([2, 0, 1])
     return ROIs
 
 def test_mrcnn():
-    weights_path = download_model('mask_rcnn')    
+    weights_path = download_model('mask_rcnn')
     summary_images = cm.load(download_demo('demo_voltage_imaging_summary_images.tif'))
     ROIs = mrcnn(img=summary_images.transpose([1, 2, 0]), size_range=[5, 22],
                                  weights_path=weights_path)
     assert ROIs.shape[0] == 14, 'fail to infer correct number of neurons'
-    
-    
\ No newline at end of file
diff --git a/caiman/utils/nn_models.py b/caiman/utils/nn_models.py
index fd1a63fc..dcf5eff4 100644
--- a/caiman/utils/nn_models.py
+++ b/caiman/utils/nn_models.py
@@ -7,7 +7,6 @@ one photon data using a "ring-CNN" background model.
 
 import numpy as np
 import os
-import tensorflow as tf
 from tensorflow.keras.layers import Input, Dense, Reshape, Layer, Activation
 from tensorflow.keras.models import Model
 from tensorflow.keras.optimizers import Adam
@@ -24,7 +23,7 @@ from caiman.paths import caiman_datadir
 class CalciumDataset(Sequence):
     def __init__(self, files, random_state=42, batch_size=32, train=True,
                  var_name_hdf5='mov', subindices=None):
-        """ 
+        """
         Create a Sequence object for Ca datasets. Not used at the moment
         """
         if isinstance(files, str):
@@ -44,7 +43,7 @@ class CalciumDataset(Sequence):
         self.train = train
         self.on_epoch_end()
         self.var_name_hdf5 = var_name_hdf5
-    
+
     def __len__(self):
         return (np.array(self.T)//self.batch_size).sum()
 
@@ -60,8 +59,8 @@ class CalciumDataset(Sequence):
         return X, X
 
     def on_epoch_end(self):
-        # loop thru files, and shuffle them up. 
-        # shuffles order for next epoch. 
+        # loop thru files, and shuffle them up.
+        # shuffles order for next epoch.
         np.random.shuffle(self.files)
 
 class Masked_Conv2D(Layer):
@@ -138,6 +137,8 @@ class Masked_Conv2D(Layer):
         super(Masked_Conv2D, self).build(input_shape)
 
     def call(self, x):
+        # Lazyload tensorflow to avoid a hard dependency on it
+        import tensorflow as tf
         #hm = tf.multiply(self.h, K.expand_dims(K.expand_dims(tf.cast(self.mask, float))))
         #hm = tf.multiply(hm, hm>0)
         #hm = tf.where(hm>0, hm, 0)
@@ -183,12 +184,14 @@ def masked_constraint(R):
 
     Args:
         R: np.array
-            Binary mask that extracts 
+            Binary mask that extracts
 
     Returns:
         my_constraint: function
             Function that enforces the constraint
     """
+    # Lazyload tensorflow to avoid a hard dependency on it
+    import tensorflow as tf
     R = tf.cast(R, dtype=tf.float32)
     R_exp = tf.expand_dims(tf.expand_dims(R, -1), -1)
     def my_constraint(x):
@@ -217,6 +220,8 @@ class Hadamard(Layer):
         super(Hadamard, self).build(input_shape)
 
     def call(self, x):
+        # Lazyload tensorflow to avoid a hard dependency on it
+        import tensorflow as tf
         hm = tf.multiply(x, self.kernel)
         sm = tf.reduce_sum(hm, axis=-1, keepdims=True)
         return sm
@@ -246,6 +251,8 @@ class Additive(Layer):
         super(Additive, self).build(input_shape)
 
     def call(self, x):
+        # Lazyload tensorflow to avoid a hard dependency on it
+        import tensorflow as tf
         hm = tf.add(x, self.kernel)
         return hm
 
@@ -264,6 +271,8 @@ def cropped_loss(gSig=0):
         my_loss: cropped loss function
     """
     def my_loss(y_true, y_pred):
+        # Lazyload tensorflow to avoid a hard dependency on it
+        import tensorflow as tf
         if gSig > 0:
             error = tf.square(y_true[gSig:-gSig, gSig:-gSig] - y_pred[gSig:-gSig, gSig:-gSig])
         else:
@@ -282,6 +291,8 @@ def quantile_loss(qnt=.50):
         my_qnt_loss: quantile loss function
     """
     def my_qnt_loss(y_true, y_pred):
+        # Lazyload tensorflow to avoid a hard dependency on it
+        import tensorflow as tf
         error = y_true - y_pred
         pos_error = error > 0
         return tf.where(pos_error, error*qnt, error*(qnt-1))
@@ -304,6 +315,8 @@ def total_variation_loss():
     """ Returns a total variation norm loss function that can be used for training.
     """
     def my_total_variation_loss(y_true, y_pred):
+        # Lazyload tensorflow to avoid a hard dependency on it
+        import tensorflow as tf
         error = tf.reduce_mean(tf.image.total_variation(y_true - y_pred))
         return error
     return my_total_variation_loss
@@ -320,7 +333,9 @@ def b0_initializer(Y, pct=10):
     Returns:
         b0_init: keras initializer
     """
-    def b0_init(shape, dtype=tf.float32):
+    def b0_init(shape, dtype=None):
+        # Lazyload tensorflow to avoid a hard dependency on it
+        import tensorflow as tf
         mY = np.percentile(Y, pct, 0)
         #mY = np.min(Y, axis=0)
         if mY.ndim == 2:
@@ -412,7 +427,7 @@ def create_LN_model(Y=None, shape=(None, None, 1), n_channels=2, gSig=5, r_facto
 def create_NL_model(Y=None, shape=(None, None, 1), n_channels=8, gSig=5, r_factor=1.5,
                     use_add=True, initializer='he_normal', lr=1e-4, pct=10,
                     activation='relu', loss='mse', width=5, use_bias=True):
-    """ Creates a two layer nonlinear convolutional neural network with ring 
+    """ Creates a two layer nonlinear convolutional neural network with ring
     shape convolutions (but no multiplicative layers). User needs to specify
     the radius of the average neuron through gSig and the number of channels.
     The other parameters can be modified or left to the default values. The
@@ -452,9 +467,9 @@ def create_NL_model(Y=None, shape=(None, None, 1), n_channels=8, gSig=5, r_facto
 
         pct: float, default: 10
             percentile used for initializing additive layer
- 
+
         activation: str or keras initializer, default: 'relu'
-            (nonlinear) activation function 
+            (nonlinear) activation function
 
         loss: str or keras loss function
             loss function used for training
@@ -489,7 +504,7 @@ def fit_NL_model(model_NL, Y, patience=5, val_split=0.2, batch_size=32,
                  epochs=500, schedule=None):
     """
     Fit either the linear or the non-linear model. The model is fit for a
-    use specified maximum number of epochs and early stopping is used based on the 
+    use specified maximum number of epochs and early stopping is used based on the
     validation loss. A Tensorboard compatible log is also created.
 
     Args:
@@ -506,7 +521,7 @@ def fit_NL_model(model_NL, Y, patience=5, val_split=0.2, batch_size=32,
         schedule: keras learning rate scheduler
 
     Returns:
-        model_NL: 
+        model_NL:
             Keras Ring-CNN model
             trained model loaded with best weights according to validation loss
         history_NL:
diff --git a/caiman/utils/utils.py b/caiman/utils/utils.py
index 54f028e3..a0b704f3 100644
--- a/caiman/utils/utils.py
+++ b/caiman/utils/utils.py
@@ -24,7 +24,6 @@ import pickle
 import scipy
 import ssl
 import subprocess
-import tensorflow as tf
 import time
 from scipy.ndimage import gaussian_filter
 from tifffile import TiffFile
@@ -53,7 +52,7 @@ def download_demo(name:str='Sue_2x_3000_40_-46.tif', save_folder:str='') -> str:
         Args:
             name: str
                 the path of the file correspondong to a file in the filelist (''Sue_2x_3000_40_-46.tif' or 'demoMovieJ.tif')
-    
+
             save_folder: str
                 folder inside ./example_movies to which the files will be saved. Will be created if it doesn't exist
         Returns:
@@ -77,7 +76,7 @@ def download_demo(name:str='Sue_2x_3000_40_-46.tif', save_folder:str='') -> str:
                 'demoMovieJ.tif': 'https://caiman.flatironinstitute.org/~neuro/caiman_downloadables/demoMovieJ.tif',
                 'demo_behavior.h5': 'https://caiman.flatironinstitute.org/~neuro/caiman_downloadables/demo_behavior.h5',
                 'demo_voltage_imaging_ROIs.hdf5': 'https://caiman.flatironinstitute.org/~neuro/caiman_downloadables/demo_voltage_imaging_ROIs.hdf5',
-                'demo_voltage_imaging.hdf5': 'https://caiman.flatironinstitute.org/~neuro/caiman_downloadables/demo_voltage_imaging.hdf5', 
+                'demo_voltage_imaging.hdf5': 'https://caiman.flatironinstitute.org/~neuro/caiman_downloadables/demo_voltage_imaging.hdf5',
                 'demo_voltage_imaging_summary_images.tif': 'https://caiman.flatironinstitute.org/~neuro/caiman_downloadables/demo_voltage_imaging_summary_images.tif',
                 'gmc_960_30mw_00001_red.tif': 'https://caiman.flatironinstitute.org/~neuro/caiman_downloadables/gmc_960_30mw_00001_red.tif',
                 'gmc_960_30mw_00001_green.tif': 'https://caiman.flatironinstitute.org/~neuro/caiman_downloadables/gmc_960_30mw_00001_green.tif',
@@ -127,7 +126,7 @@ def download_model(name:str='mask_rcnn', save_folder:str='') -> str:
         Args:
             name: str
                 the path of the file correspondong to a file in the filelist
-    
+
             save_folder: str
                 folder inside caiman_data/model to which the files will be saved. Will be created if it doesn't exist
         Returns:
@@ -351,22 +350,22 @@ def apply_magic_wand(A, gSig, dims, A_thr=None, coms=None, dview=None,
     Args:
         A:
             output of CNMF
-    
+
         gSig: tuple
             input of CNMF (half neuron size)
-    
+
         A_thr:
             thresholded version of A
-    
+
         coms:
             centers of the magic wand
-    
+
         dview:
             for parallelization
-    
+
         min_frac:
             fraction of minimum of gSig to take as minimum size
-    
+
         max_frac:
             multiplier of maximum of gSig to take as maximum size
 
@@ -617,6 +616,8 @@ def parmap(f, X, nprocs=multiprocessing.cpu_count()):
 
 def load_graph(frozen_graph_filename):
     """ Load a tensorflow .pb model and use it for inference"""
+    # Lazy load tensorflow to avoid hard dependency on it.
+    import tensorflow as tf
     # We load the protobuf file from the disk and parse it to retrieve the
     # unserialized graph_def
     with tf.gfile.GFile(frozen_graph_filename, "rb") as f:
@@ -667,7 +668,7 @@ def get_caiman_version() -> tuple[str, str]:
             for line in sfh:
                 if ':' in line: # expect a line like "Version:1.3"
                     _, version = line.rstrip().split(':')
-                    return 'RELEASE', version 
+                    return 'RELEASE', version
 
     # Attempt: 'FILE'
     # Right now this samples the utils directory
